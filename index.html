
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>

  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 400;
src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: italic;
font-weight: 700;
src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 400;
src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}
/* latin-ext */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
}
/* latin */
@font-face {
font-family: 'Lato';
font-style: normal;
font-weight: 700;
src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
}

a {
color: #1772d0;
text-decoration: none;
}

a:focus,
a:hover {
color: #f09228;
text-decoration: none;
}

body,
td,
th,
tr,
p,
a {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
text-align: justify;
text-justify: inter-word;
}

strong {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
}

heading {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 24px;
text-decoration: underline;
}

papertitle {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 14px;
font-weight: 700
}

name {
font-family: 'Lato', Verdana, Helvetica, sans-serif;
font-size: 36px;
}

.one {
width: 160px;
height: 160px;
position: relative;
}

.two {
width: 160px;
height: 160px;
position: absolute;
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

.fade {
transition: opacity .2s ease-in-out;
-moz-transition: opacity .2s ease-in-out;
-webkit-transition: opacity .2s ease-in-out;
}

span.highlight {
background-color: #ffffd0;
}
  </style>
  <link rel="icon" type="image/png" href="img/avtr.png">
  <title>Aalok Gangopadhyay</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>

  </head>
  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="78%" valign="middle">
        <p align="center">
          <name>Aalok Gangopadhyay</name>
        </p>
        <p>
          I am a Visiting Fellow at the Tata Institute of Fundamental Research, India, 
          advised by <a href="https://www.tifr.res.in/~hariharan.narayanan/" target="_blank">Prof. Hariharan Narayanan</a>. 
          I completed my Ph.D. in Electrical Engineering at the Indian Institute of Technology, Gandhinagar, India, 
          under the mentorship of <a href="https://people.iitgn.ac.in/~shanmuga/index.html" target="_blank">Prof. Shanmuganathan Raman</a>.
          My research interests include Computer Graphics, Computer Vision, Deep Learning, Shape Analysis, Manifold Learning and Algebraic Combinatorics.
        </p>
        
        <p align=center>
          <a href="mailto:aalok1993@gmail.com">Email</a> &nbsp/&nbsp
          <a href="https://aalok1993.github.io/pdf/cv.pdf" target="_blank">CV</a> &nbsp/&nbsp
	        <a href="https://scholar.google.com/citations?user=Vb6eY0MAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
          <a href="https://github.com/aalok1993" target="_blank">GitHub</a> &nbsp/&nbsp
          <a href="https://www.linkedin.com/in/aalok-gangopadhyay-050a1b282/" target="_blank">LinkedIn</a>
        </p>

        </td>
        <td width="33%">
          <a href="img/avtr.png">
              <img src="img/avtr-circle1.png" width="300px">
          </a>
        </td>
      </tr>
    </table>

      <!-- Research Publications -->
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publications and Preprints</heading>
        </td>
      </tr>
      </table>



  <!-- Hives -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
            <div class="two" id='dd_image'>
                <a href='teaser/hives.png'><img src='teaser/hives.png' width="160"></a>
            </div>
            <a href='teaser/hives.png'><img src='teaser/hives.png' width="160"></a>
        </div>
    </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2410.12619" target="_blank">
          <papertitle>On the Randomized Horn Problem and the Surface Tension of Hives</papertitle>
        </a>
        <br>
        <strong>Aalok Gangopadhyay</strong>,
        Hariharan Narayanan.
         <br>
        <p>
          [<a href="https://arxiv.org/abs/2410.12619" target="_blank">paper</a>] 
          [<a href="https://github.com/aalok1993/combinatorial-hives.git" target="_blank">code</a>]
          [<a href="https://docs.google.com/presentation/d/e/2PACX-1vRrLTGyBuGElC-skqXVXYfU_n54encdmaZEANsOtdU6U772WsYcDoYd8q1YngkmJgnOZxkeSyFLLi4E/pub?start=false&loop=false&delayms=3000" target="_blank">slides</a>]
          [<a href="https://www.youtube.com/watch?v=sDI_kSXTV6E&ab_channel=STCSTIFR" target="_blank">talk</a>]
        </p>
        <p>We investigate the Horn problem and its randomized version, which involve determining the possible spectra of sums of Hermitian matrices with given spectra, using the hives framework introduced by Knutson and Tao. 
          Building on previous work by Narayanan and Sheffield, we provide upper and lower bounds on the surface tension function associated with large deviations, and derive a closed-form expression for the total entropy of a surface tension minimizing continuum hive with boundary conditions arising from Gaussian Unitary Ensemble eigenspectra. 
          Additionally, we present empirical results for random hives and lozenge tilings obtained via the octahedron recurrence, along with a numerical approximation of the surface tension function.</p>
      </td>
    </tr>

  <!-- Flow Symmetrization -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one">
            <div class="two" id='dd_image'>
                <a href='teaser/flowsymm.png'><img src='teaser/flowsymm.png' width="160"></a>
            </div>
            <a href='teaser/flowsymm.png'><img src='teaser/flowsymm.png' width="160"></a>
        </div>
    </td>
      <td style="padding:20px;width:75%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2312.06317" target="_blank">
          <papertitle>Flow Symmetrization for Parameterized Constrained Diffeomorphisms</papertitle>
        </a>
        <br>
        <strong>Aalok Gangopadhyay</strong>,
        Dwip Dalal, 
        Progyan Das, 
        Shanmuganathan Raman.
         <br>
         <p>
          [<a href="https://arxiv.org/abs/2312.06317" target="_blank">paper</a>] 
          [<a href="" target="_blank">code</a>]
          [<a href="https://docs.google.com/presentation/d/e/2PACX-1vR2QRvwzFJ9893Wq6riQJWkMIWcYxYD3Kknt4DAB4RlGR-5anGOA1mqu_wkt0GTlYU2i0tvxHymZtfT/pub?start=false&loop=false&delayms=3000" target="_blank">slides</a>]
        </p>
        <p> We introduce Flow Symmetrization, a method to represent parametric families of constrained diffeomorphisms with symmetry constraints such as periodicity, rotation equivariance, and transflection equivariance. 
          This differentiable approach is suitable for gradient-based optimization and ideal for representing tile shapes in tiling classes, addressing problems like Escherization and Density Estimation. 
          Our experiments show impressive results for deforming tile shapes to match target shapes on the Euclidean plane and performing density estimation on non-Euclidean identification spaces like torus, sphere, Klein bottle, and projective plane.</p>
      </td>
    </tr>



    <!-- Knot Art -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
              <div class="two" id='dd_image'>
                  <a href='teaser/knotart.png'><img src='teaser/knotart.png' width="160"></a>
              </div>
              <a href='teaser/knotart.png'><img src='teaser/knotart.png' width="160"></a>
          </div>
      </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://diglib.eg.org/items/3882b329-ee4e-467e-9f93-4acd0eda30b2" target="_blank">
            <papertitle>Search Me Knot, Render Me Knot: Embedding Search and Differentiable Rendering of Knots in 3D</papertitle>
          </a>
          <br>
          <strong>Aalok Gangopadhyay</strong>,
          Paras Gupta,
          Tarun Sharma,
          Prajwal Singh,
          Shanmuganathan Raman.
           <br>
           <p>
            [<a href="https://diglib.eg.org/items/3882b329-ee4e-467e-9f93-4acd0eda30b2" target="_blank">paper</a>]
            [<a href="https://github.com/aalok1993/KnotArt.git" target="_blank">code</a>]
            [<a href="https://docs.google.com/presentation/d/e/2PACX-1vRKbwWFhLPGsCC03bR-bruMvlMzsLL_C0Mo7BKTdEZIjjITQMRB-HTjL1vHuWluhGHgHWJ8gBoNfWih/pub?start=false&loop=false&delayms=3000" target="_blank">slides</a>]
            [<a href="https://www.youtube.com/watch?v=dPgIB0oJ66c&t=1470s" target="_blank">talk</a>]
          </p>
          <p> We address the problem of creating knot-based inverse perceptual art by finding a 3D tubular structure whose projection matches multiple target images from specified viewpoints. 
            Our method involves a differentiable rendering algorithm and gradient-based optimization to search for ideal knot embeddings, using homeomorphisms parametrized by an invertible neural network. 
            We impose physical constraints through loss functions to ensure the tube is feasible, self-intersection-free, and cost-effective, demonstrating the effectiveness of our approach with both simulations and a real-world 3D-printed example. </p>
        </td>
      </tr>



    <!-- Hand Shadow Art -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
              <div class="two" id='dd_image'>
                  <a href='teaser/handshadowart.png'><img src='teaser/handshadowart.png' width="160"></a>
              </div>
              <a href='teaser/handshadowart.png'><img src='teaser/handshadowart.png' width="160"></a>
          </div>
      </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://diglib.eg.org/items/6df08ed4-7713-4ae5-9217-f03e9961e697" target="_blank">
            <papertitle>Hand Shadow Art: A Differentiable Rendering Perspective</papertitle>
          </a>
          <br>
          <strong>Aalok Gangopadhyay</strong>,
          Prajwal Singh, 
          Ashish Tiwari,
          Shanmuganathan Raman.
           <br>
           <p>
            [<a href="https://diglib.eg.org/items/6df08ed4-7713-4ae5-9217-f03e9961e697" target="_blank">paper</a>]
            [<a href="https://drive.google.com/file/d/19XX9OCaP0t8Yb-yBIWrFr9LtmwqgABs2/view?usp=drive_link" target="_blank">poster</a>]
            [<a href="" target="_blank">code</a>]
            </p>
          <p>Shadow art creates captivating effects through 2D shadows cast by 3D shapes, with hand shadows (or shadow puppetry) involving the use of hands and fingers to form meaningful silhouettes on a wall. 
            This work introduces a differentiable rendering-based approach to deform hand models, enabling them to cast shadows that match a desired target image and lighting configuration. 
            We demonstrate results with shadows cast by two hands and interpolate hand poses between target shadow images, offering a valuable tool for the graphics community.</p>
        </td>
      </tr>


    <!-- Mesh Dynamics -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
              <div class="two" id='dd_image'>
                  <a href='teaser/meshdynamics.png'><img src='teaser/meshdynamics.png' width="160"></a>
              </div>
              <a href='teaser/meshdynamics.png'><img src='teaser/meshdynamics.png' width="160"></a>
          </div>
      </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2306.13452" target="_blank">
            <papertitle>A Graph Neural Network Approach for Temporal Mesh Blending and Correspondence</papertitle>
          </a>
          <br>
          <strong>Aalok Gangopadhyay</strong>,
          Abhinav Narayan Harish,
          Prajwal Singh,
          Shanmuganathan Raman.
           <br>
           <p>
            [<a href="https://arxiv.org/abs/2306.13452" target="_blank">paper</a>]
            [<a href="" target="_blank">code</a>]
            [<a href="https://docs.google.com/presentation/d/e/2PACX-1vQFz1yoCGqLCRJ0yB2ta17aIz1i5JKSXoGDHm7VBJ4g9YUDBlTHYA2LGNIsHZ4hZBUnKJ5Fx9bn0kux/pub?start=false&loop=false&delayms=3000" target="_blank">slides</a>]
            </p>
          <p>We propose a self-supervised deep learning framework for mesh blending when meshes lack correspondence, using Red-Blue MPNN, a novel graph neural network that estimates correspondence via an augmented graph. 
            We introduce a conditional refinement scheme to find exact correspondence under certain conditions and develop another graph neural network to process aligned meshes and time values to generate desired results. 
            Our large-scale synthetic dataset, created using motion capture and human mesh designing software, shows that our approach produces realistic body part deformations from complex inputs.</p>
        </td>
      </tr>


    <!-- APEX-Net -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
              <div class="two" id='dd_image'>
                  <a href='teaser/APEX-Net.png'><img src='teaser/APEX-Net.png' width="160"></a>
              </div>
              <a href='teaser/APEX-Net.png'><img src='teaser/APEX-Net.png' width="160"></a>
          </div>
      </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/9806720" target="_blank">
            <papertitle>APEX-Net: Automatic Plot Extraction Network</papertitle>
          </a>
          <br>
          <strong>Aalok Gangopadhyay</strong>,
          Prajwal Singh,
          Shanmuganathan Raman.
           <br>
           <p>
            [<a href="https://ieeexplore.ieee.org/abstract/document/9806720" target="_blank">paper</a>]
            [<a href="https://github.com/aalok1993/APEX-Net.git" target="_blank">code</a>]
            [<a href="https://docs.google.com/presentation/d/e/2PACX-1vTvUfmwAEIFSbO4FA1uYF6RCJrTPpy920XU3CYuSbiuBpqVnbvNLMTDMEh7RlU07XSxfJW86gNLhDye/pub?start=false&loop=false&delayms=3000" target="_blank">slides</a>]
            [<a href="https://drive.google.com/file/d/1ScX7b_s7gOfijAZSdBOKWA2Rn8vzBBxN/view?usp=drive_link" target="_blank">talk</a>]
            </p>
          <p>Automatic plot extraction involves identifying and extracting individual line plots from images containing multiple 2D line plots, a problem with many real-world applications that typically requires significant human intervention. 
            To reduce this need, we propose APEX-Net, a deep learning framework with novel loss functions, and introduce APEX-1M, a large-scale dataset containing plot images and raw data. 
            APEX-Net demonstrates impressive accuracy on the APEX-1M test set and effectively extracts plot shapes from unseen images, with a GUI developed for community use.</p>
        </td>
      </tr>


    <!-- DMD-Net -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
              <div class="two" id='dd_image'>
                  <a href='teaser/DMD-Net.png'><img src='teaser/DMD-Net.png' width="160"></a>
              </div>
              <a href='teaser/DMD-Net.png'><img src='teaser/DMD-Net.png' width="160"></a>
          </div>
      </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/abstract/document/9956054" target="_blank">
            <papertitle>DMD-Net: Deep Mesh Denoising Network</papertitle>
          </a>
          <br>
          <strong>Aalok Gangopadhyay</strong>,
          Shashikant Verma,
          Shanmuganathan Raman.
           <br>
           <p>
            [<a href="https://ieeexplore.ieee.org/abstract/document/9956054" target="_blank">paper</a>]
            [<a href="https://docs.google.com/presentation/d/e/2PACX-1vRpgt3KqJsXLoShgieFofyT8y5REtX0cYEgQza_M55I62xsjy-0ELQ5ReYS8XJOqnnOVoB8aIkYYthk/pub?start=false&loop=false&delayms=3000" target="_blank">slides</a>]
            [<a href="https://drive.google.com/file/d/1BE-OcUGGYt3PfMUjsqC_htr9VAbI2qgC/view?usp=drive_link" target="_blank">talk</a>]
            </p>
          <p>We introduce Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning framework for mesh denoising, utilizing a Graph Convolutional Neural Network with aggregation in both primal and dual graphs through an asymmetric two-stream network with a primal-dual fusion block. 
            The Feature Guided Transformer (FGT) paradigm, comprising a feature extractor, transformer, and denoiser, guides the transformation of noisy input meshes to achieve useful intermediate representations and ultimately denoised meshes. 
            Trained on a large dataset of 3D objects, our method demonstrates competitive or superior performance compared to state-of-the-art algorithms, showing robustness even against extremely high noise levels.</p>
        </td>
      </tr>


    <!-- Motion Deblurring -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
              <div class="two" id='dd_image'>
                  <a href='teaser/deblurring.png'><img src='teaser/deblurring.png' width="160"></a>
              </div>
              <a href='teaser/deblurring.png'><img src='teaser/deblurring.png' width="160"></a>
          </div>
      </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/document/8265564" target="_blank">
            <papertitle>Deep Generative Filter for Motion Deblurring</papertitle>
          </a>
          <br>
          Sainandan Ramakrishnan, 
          Shubham Pachori,
          <strong>Aalok Gangopadhyay</strong>,
          Shanmuganathan Raman.
           <br>
          <p>
            [<a href="https://ieeexplore.ieee.org/document/8265564" target="_blank">paper</a>]
            [<a href="https://github.com/sainandan-ramakrishnan/Deep-Generative-Filter-for-motion-deblurring.git" target="_blank">code</a>]
            [<a href="https://drive.google.com/file/d/1qc-ytINZJwpt1xZx-eorBqua5FcNAni-/view?usp=drive_link" target="_blank">slides</a>]
          </p>
          <p>Removing blur caused by camera shake is challenging due to its ill-posed nature, with motion blur inducing a spatially varying effect. 
            This paper proposes a novel deep filter using Generative Adversarial Network (GAN) architecture with global skip connection and dense architecture to address this issue without needing blur kernel estimation, thus reducing test time for practical use. 
            Experiments on benchmark datasets demonstrate that the proposed method outperforms state-of-the-art blind deblurring algorithms both quantitatively and qualitatively.</p>
        </td>
      </tr>

    <!-- Automatic silhouette photography -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
              <div class="two" id='dd_image'>
                  <a href='teaser/autosillphoto.png'><img src='teaser/autosillphoto.png' width="160"></a>
              </div>
              <a href='teaser/autosillphoto.png'><img src='teaser/autosillphoto.png' width="160"></a>
          </div>
      </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/document/7561153" target="_blank">
            <papertitle>Automatic silhouette photography</papertitle>
          </a>
          <br>
          <strong>Aalok Gangopadhyay</strong>,
          Shubham Pachori,
          Shanmuganathan Raman.
           <br>
           <p>
            [<a href="https://ieeexplore.ieee.org/document/7561153" target="_blank">paper</a>]
            [<a href="https://docs.google.com/presentation/d/e/2PACX-1vSR8U7CFtewERBUHnojbWroTlKF7_heeR7HAU_1tBQUFU75s1bVRsiWw236ApuqTExsV-UvgkjA6EmH/pub?start=false&loop=false&delayms=3000" target="_blank">slides</a>]
            [<a href="https://drive.google.com/file/d/1dcYxZfcNmHGDcwosRAk2e1cS9PJciJFP/view?usp=sharing" target="_blank">poster</a>]
          </p>
          <p>This paper addresses the challenge of automatically generating silhouette images from natural scene photographs. 
            The key technique involves darkening the foreground and illuminating the background to achieve the silhouette effect. 
            The proposed computational pipeline successfully produces high-quality silhouette images from any given image with a foreground object and background.</p>
        </td>
      </tr>
      
    <!-- Dynamic scene classification -->
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr onmouseout="bhnerf_stop()" onmouseover="bhnerf_start()">
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
              <div class="two" id='dd_image'>
                  <a href='teaser/sacnn.png'><img src='teaser/sacnn.png' width="160"></a>
              </div>
              <a href='teaser/sacnn.png'><img src='teaser/sacnn.png' width="160"></a>
          </div>
      </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://ieeexplore.ieee.org/document/7906042" target="_blank">
            <papertitle>Dynamic scene classification using convolutional neural networks</papertitle>
          </a>
          <br>
          <strong>Aalok Gangopadhyay</strong>,
          Shivam Mani Tripathi, 
          Ishan Jindal, 
          Shanmuganathan Raman.
           <br>
          <p>
          [<a href="https://ieeexplore.ieee.org/document/7906042" target="_blank">paper</a>]
          </p>
          <p> Classifying videos of natural dynamic scenes, especially with dynamic cameras, is challenging. 
            This paper analyzes statistical aggregation techniques on pre-trained convolutional neural network (CNN) models to create robust feature descriptors from CNN activation features across video frames. 
            Results show that this approach outperforms state-of-the-art methods on the Maryland and YUPenn datasets, effectively distinguishing dynamic scenes even with dominant camera motion and complex dynamics, and includes an extensive comparison of various aggregation methods.</p>
        </td>
      </tr>












    </td>
    </tr>
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <br>
        <p align="right">
          <font size="2">
          Thanks to <a href="https://jonbarron.info" >Jon Barron</a> for this awesome template and <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a> for additional formatting.<br>
          Last updated November 2024.
	    </font>
        </p>
        </td>
      </tr>
    </table>

  </table>
  </body>
</html>
